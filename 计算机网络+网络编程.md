# 计算机网络+网络编程

## 1. TCP首部

![Image text](https://github.com/Arzinia/MyProject/blob/master/images/TCP%E9%A6%96%E9%83%A8.png)

## 2. 三次握手

![Image text](https://github.com/Arzinia/MyProject/blob/master/images/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.png)

1、client发送SYN到server，将状态修改为SYN_SEND，如果server收到请求，则将状态修改为SYN_RCVD，并把该请求放到syns queue队列中

2、server回复SYN+ACK给client，如果client收到请求，则将状态修改为ESTABLISHED，并发送ACK给server

3、server收到ACK，将状态修改为ESTABLISHED，并把该请求从syns queue中放到accept queue

在linux系统内核中维护了两个队列：syns queue和accept queue

（1）服务端只处理指定来源IP的TCP连接请求，其它未指定来源的连接请求一概拒绝

（2）缩短服务端的等待时间

## 3. 四次挥手

![Image text](https://github.com/Arzinia/MyProject/blob/master/images/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B.png)

1. A向B发送一个FIN标志为1的报文

2. B接收到这个FIN报文，回复一个ACK，进入CLOSE_WAIT状态，等待断开接收方的连接，此时B可以继续发送报文

3. A接收到ACK，处于半关闭状态，但是还是可以接收

4. B的应用层决定断开连接，此时B向A发送FIN

5. A接收到FIN ，回复一个ACK，进入TIME_WAIT，持续2个MSL (1~4分钟)后才能进入CLOSED状态

6. B接收到ACK，立刻进入CLOSED状态

   

为什么要等两个MSL？

1.最后一个ACK可能会丢失，如果ACK丢失了，B会超时重传FIN的，如果A接收到这个重传的FIN，则又会发一次ACK，并重新计时两个MSL

2.等待这次连接的所有报文都消失在网络里，防止已经失效的请求出现引起不必要的错误

## 4. TCP和UDP的区别

1、TCP面向连接，发送之前需要先建立连接；UDP是无连接的，即发送数据之前不需要建立连接

2、TCP提供可靠的服务，数据无差错，不丢失，不重复，且按序到达；UDP尽最大努力交付，即不保证可靠交付

3、TCP面向字节流，实际上是TCP把数据看成一连串无结构的字节流，会把字节流切分成窗口大小；UDP是面向报文,给多少发多少

4、UDP没有拥塞控制，和流量控制。因此网络出现拥塞不会使源主机的发送速率降低

5、每一条TCP连接只能是点到点的；UDP支持一对一，一对多，多对一和多对多的交互通信

6、TCP首部开销20字节;UDP的首部开销小，只有8个字节

1:面向报文

面向报文的传输方式是应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文。因此，应用程序必须选择合适大小的报文。若报文太长，则IP层需要分片，降低效率。若太短，会是IP太小。UDP对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。这也就是说，应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文

2.面向字节流

TCP把应用程序看成是一连串的无结构的字节流。TCP有一个缓冲，当应用程序传送的数据块太长，TCP就可以把它划分短一些再传送。如果应用程序一次只发送一个字节，TCP也可以等待积累有足够多的字节后再构成报文段发送出去

## 5. TCP可靠性保证

1、确认应答机制  每收到一个报文都会回复相应的ack

2、超时重传机制  包括定时重传和快重传

3、流量控制      收端和发端分别维护一个接收窗口和发送窗口，发送窗口大小不能大于接收窗口，发送窗口大小的维护由确认报文中的窗口大小字段来确定，建立tcp连接的时候，接收端会在ACK报文中告诉发送端接收端的接收窗口大小，发送窗口不能大于这个值

4、拥塞控制      发送端维护一个叫做拥塞窗口cwnd的状态量，拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化，发送方让自己的发送窗口等于拥塞窗口，并配合几种拥塞避免算法cwnd初始值为2~4个SMSS(Sender Maximum Segment Size 最大报文段长度)，规定了初始拥塞窗口的字节数。Linux3.0之后，拥塞窗口为10个mss

## 6. 拥塞避免算法

**1.慢开始**

指数增长，开始的时候把拥塞窗口设置为1，每经过一个往返时间RTT，拥塞窗口就加倍(也可以说是传输轮次，意思就是把拥塞窗口里的数据全都发送完毕并且收到最后一个ack)为了防止拥塞窗口过大引起网络拥塞，还要设定一个慢启动阈值ssthresh（慢启动阈值其实为无穷大，但是如果大于流量控制窗口就没意义了）。发生超时，则把慢启动阈值设为当前窗口的一半，并重新开始慢启动。当拥塞窗口<阈值  使用慢启动算法（指数增加）当拥塞窗口>阈值  使用拥塞避免算法（线性增加）

**2.拥塞避免**

当慢启动过程中拥塞窗口到达慢启动阈值ssthresh时，改用拥塞避免算法，每经过一个往返时间RTT，拥塞窗口就加1当出现网络拥塞(超时重传)，则立刻把慢启动阈值ssthresh设定为当前拥塞窗口cwnd一半，然后重新开始慢启动（要记得拥塞避免永远是从慢启动门限ssthresh开始执行的）如果超时-慢启动如果丢包-快恢复

**3.快重传**

要求接收方对接收到的报文立即进行确认，收到失序的报文也要对按序到达的报文进行确认（M1,M2收到，M3未收到，M4收到，此时不能发出对M4的确认，应该继续发出对M2的确认），这样发送方接收到三个连续的ack就知道丢包了，不会以为是网络拥塞，此时会立即进行重传

**4.快恢复**

发送方知道只是丢失了个别报文，而不是拥塞，于是不启动慢开始，而是执行快恢复，此时设置慢启动阈值ssthresh为当前拥塞窗口cwnd的一半，并且把拥塞窗口cwnd设置=慢启动阈值，然后执行拥塞避免算法

## 7. TCP粘包拆包问题

应用层首先要将自己的数据通过套接字发送，首先要调用一个write方法：（将应用进程缓冲区中的数据拷贝到套接口发送缓冲区SO_SNDBUF，有一个大小的限制），如果应用进程缓冲区的一条消息的字节的大小超过了发送缓冲区的大小，就有可能产生粘包问题，因为消息已经被分割了，有可能一部分已经被发送出去了，对方已经接受了，但是另外一部分可能刚放入套接口发送缓冲区里准备进一步发送，就直接导致接受的后一部分，直接导致了粘包问题的出现。
就是说：TCP是基于字节流的，只维护发送出去多少，确认了多少，并没有维护消息与消息之间的边界，因而极有可能导致粘包问题，（应该在应用层维护一个消息边界，加\n）TCP所发送的的字节流中存在一个MSS（最大报文端长度），如果所发送的消息的字节过长，那么会对所发送的消息进行分割，那么也会直接导致粘包
链路层所发送的数据有一个最大传输单元（MTU）的限制（以太网的MTU是1500bytes），如果我们所传输的信息超过了限制，那么会在IP层进行分组，或者分片，这也可能导致消息的粘包问题的产生

## 8. 如何降低TCP时延

双缓冲消息队列-减少锁竞争

在网络应用服务器端, 为了性能和防止阻塞, 经常会把逻辑处理和I/O处理分离:I/O网络线程处理I/O事件: 数据包的接收和发送, 连接的建立和维护等.逻辑线程要对收到的数据包进行逻辑处理.通常网络线程和逻辑线程之间是通过数据包队列来交换信息, 简单来说就是一个生产者-消费者模式.这个队列是多个线程在共享访问必须加锁, 意味着每次访问都要加锁。如何更好的如何减少锁竞争次数呢 ?

方案一 双缓冲消息队列:两个队列，一个给逻辑线程读，一个给IO线程用来写，当逻辑线程读完队列后会将自己的队列与IO线程的队列相调换。IO线程每次写队列时都要加锁，逻辑线程在调换队列时也需要加锁，但逻辑线程在读队列时是不需要加锁的.队列缓冲区的大小要根据数据量的大小进行调整的，如果缓冲区很小，就能更及时的处理数据，但吞吐量以及出现资源竞争的几率大多了。可以给缓冲队列设置最大上限，超过上限的数量之后，将包丢弃不插入队列。另外，双缓冲的实现也有不同策略的，一是读操作优先，就是生产者只要发现空闲缓冲，马上swap，二是写线程只有在当前的缓冲区写满了，才进行swap操作。三是上层逻辑按照帧率来处理，每一帧的时候将双层缓冲队列调换一下，取一个队列来处理即可.

方案二 提供一个队列容器:提供一个队列容器，里面有多个队列，每个队列都可固定存放一定数量的消息。网络IO线程要给逻辑线程投递消息时，会从队列容器中取一个空队列来使用，直到将该队列填满后再放回容器中换另一个空队列。而逻辑线程取消息时是从队列容器中取一个有消息的队列来读取，处理完后清空队列再放回到容器中。这样便使得只有在对队列容器进行操作时才需要加锁，而IO线程和逻辑线程在操作自己当前使用的队列时都不需要加锁，所以锁竞争的机会大大减少了。这里为每个队列设了个最大消息数，看来好像是打算只有当IO线程写满队列时才会将其放回到容器中换另一个队列。那这样有时也会出现IO线程未写满一个队列，而逻辑线程又没有数据可处理的情况，特别是当数据量很少时可能会很容易出现[这个可以通过设置超时来处理, 如果当前时间-向队列放入第一个包的时间> 50 ms, 就将其放回到容器中换另一个队列]。通常我们逻辑服务器会以场景来划分线程,不同线程执行不同场景.一个线程可以执行多个场景.因为玩家属于场景,我们会把玩家数据,包括其缓冲池丢给场景 去处理.

## 9. http请求报文及响应报文

![Image text](https://github.com/Arzinia/MyProject/blob/master/images/http%E8%AF%B7%E6%B1%82%E6%8A%A5%E6%96%87.png)

GET：当客户端要从服务器中读取某个资源时，使用GET 方法。GET 方法要求服务器将URL 定位的资源放在响应报文的数据部分，回送给客户端，即向服务器请求某个资源。使用GET 方法时，请求参数和对应的值附加在 URL 后面，利用一个问号(“?”)代表URL 的结尾与请求参数的开始，传递参数长度受浏览器长度限制。POST：当客户端给服务器提供信息较多时可以使用POST 方法，POST 方法向服务器提交数据，比如完成表单数据的提交，将数据提交给服务器处理。GET 一般用于获取/查询资源信息，POST 会附带用户数据，一般用于更新资源信息。POST 方法将请求参数封装在HTTP 请求数据中，以名称/值的形式出现，可以传输大量数据;POST 方法适用于需要客户填写表单的场合

![Image text](https://github.com/Arzinia/MyProject/blob/master/images/http%E5%93%8D%E5%BA%94%E6%8A%A5%E6%96%87.png)

什么是长连接、短连接？

HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。而从HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码：Connection:keep-alive

## 10. get,post方法区别

1.get是幂等的，主要是获取服务器上的资源，反复读取不应该对访问的数据有副作用。比如get一下，用户就下单了，返回订单已受理，这肯定是不现实的。在页面里<form> 标签会定义一个表单。点击其中的submit元素会发出一个post请求让服务器做一件事。这件事往往是有副作用的，不幂等的

2.get数据在url里 post数据在请求体里

3.get请求的url长度不能太长，浏览器对url长度有限制，post数据长度无限制

4.get请求可以被缓存，这个缓存可以做到浏览器本身上，也可以做到代理上（如nginx），或者做到server端。post不能，试想一下，如果post请求被浏览器缓存了，那么下单请求就可以不向服务器发请求，而直接返回本地缓存的“下单成功界面”，却又没有真的在服务器下单

5.get比post更不安全，因为参数直接暴露在URL上，所以不能用来传递敏感信息

## 11. cookie和session的区别

实现机制：Session的实现常常依赖于Cookie机制，通过Cookie机制回传SessionID；

大小限制：Cookie有大小限制并且浏览器对每个站点也有cookie的个数限制，Session没有大小限制，理论上只与服务器的内存大小有关；

安全性：Cookie存在安全隐患，通过拦截或本地文件找得到cookie后可以进行攻击，而Session由于保存在服务器端，相对更加安全；

服务器资源消耗：Session是保存在服务器端上会存在一段时间才会消失，如果session过多会增加服务器的压力

Application（ServletContext）：与一个Web应用程序相对应，为应用程序提供了一个全局的状态，所有客户都可以使用该状态

## 12. SQL注入

SQL注入就是通过把SQL命令插入到Web表单提交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令使用预编译手段，绑定参数是最好的防SQL注入的方法

## 13. https建立连接的过程

1. 客户端与服务端建立http连接
2. 服务端发送数字证书给客户端，数字证书中有公钥，由CA颁发
3. 客户端收到公钥A后，生成一组随机数，作为对称加密的密钥B
4. 客户端用公钥A对密钥B进行加密，形成密钥C，发送给服务端
5. 服务端用私钥D对收到的密钥C进行解密，获得随机数或者说密钥B
6. 双方用密钥B进行对称加密通信

## 14. 浏览器打开网页的全过程

**1.DNS解析:** 首先查询浏览器自带的DNS缓存，如果浏览器没有，那就查看本地host文件，再没有，就向本地DNS服务器发起请求，如果还没有，则域名解析服务器会向根服务器发起解析请求根DNS服务器收到请求后会判断这个域名(.com)是谁来授权管理，并会返回一个负责该顶级域名服务器的一个IP。本地DNS服务器收到IP信息后，将会联系负责.com域的这台服务器。这台负责.com域的服务器收到请求后，如果自己无法解析，它就会找一个管理.com域的下一级DNS服务器地址([http://taobao.com](http://taobao.com/))给本地DNS服务器。当本地DNS服务器收到这个地址后，就会找[http://taobao.com](http://taobao.com/)域服务器，重复上面的动作，进行查询，直至找到[www.taobao.com](http://www.taobao.com/)主机
为什么是递归：如果主机所询问的本地域名服务器不知道被查询的域名的IP地址，那么本地域名服务器就以DNS客户的身份，向其它根域名服务器继续发出查询请求报文(即替主机继续查询)，而不是让主机自己进行下一步查询。为什么是迭代：当根域名服务器收到本地域名服务器发出的迭代查询请求报文时，要么给出所要查询的IP地址，要么告诉本地服务器：“你下一步应当向哪一个域名服务器进行查询”。然后让本地服务器进行后续的查询。根域名服务器通常是把自己知道的顶级域名服务器的IP地址告诉本地域名服务器，让本地域名服务器再向顶级域名服务器查询。顶级域名服务器在收到本地域名服务器的查询请求后，要么给出所要查询的IP地址，要么告诉本地服务器下一步应当向哪一个权限域名服务器进行查询
为什么DNS用的是UDP：响应时间快，在很多时候，用户在访问一些冷门网站时，由于DNS服务器没有冷门网站的解析缓存，需要到域名根服务器、一级域名服务器、二级域名服务器迭代查询，直到查询到冷门网站的权威服务器，这中间可能涉及到多次的查询。如果使用TCP传输，多几次查询，就多几次TCP连接时间，这多出来的时间不容小觑。
DNS的缺点：1.短，由于历史的原因，互联网上物理链路的最小MTU = 576，基于UDP传输的DNS为了限制报文不超过576，所以将DNS报文限制在512字节。如果超过512，返回的DNS应答就是不完整的。2、不安全，为了克服这种困难，最简单的方式就是使用TCP，来重新查询。尽管交易时间可能比较长，但毕竟可以得到完整的答案，总比得到不完整的答案要好

**2.TCP连接 **

解析到IP地址后，浏览器发起建立TCP连接的过程，浏览器会选择一个大于1024的本机端口向目标IP地址的80端口发起TCP连接请求。经过标准的TCP握手流程，建立TCP连接

**3.发起http请求 ** 

其本质是在建立起的TCP连接中，按照HTTP协议标准发送一个索要网页的请求
**4.负载均衡**

1) 如果配备了负载均衡的话，前一步DNS解析获得的IP地址应该是我们Nginx负载均衡服务器的IP地址。所以，我们的浏览器将我们的网页请求发送到了Nginx负载均衡服务器上

2) Nginx根据我们设定的分配算法和规则，选择一台后端的真实Web服务器，与之建立TCP连接、并转发我们浏览器发出去的网页请求

**5.后端处理完成以后返回数据**

**6.浏览器渲染**  

如果浏览器解析页面内容的时候，会发现页面引用了其他未加载的image等静态内容，浏览器根据url加载该url下的图片内容。本质上是浏览器重新开始第一部分的流程，区别只是服务器不再是应用服务器，而是提供静态资源的服务器

## 15. http状态码

**200("OK")：**

表示服务器成功执行了客户端所请求的动作

**201("Created")：**

当服务器依照客户端的请求创建了一个新资源时，发送此响应代码

**202("Accepted")：**

客户端的请求无法或将不被实时处理。请求稍后会被处理。请求看上去是合法的，但在实际处理它时有出现问题的可能
**300("Multiple Choices")：**

若被请求的资源在服务器端存在多个表示，而服务器不知道客户端想要的是哪一个表示时，发送这个响应代码

**301("Moved Permanently"):**

永久重定向 服务器知道客户端试图访问的是哪个资源，但它不喜欢客户端用当前URI来请求该资源。它希望客户端记住另一个URI，并在今后的请求中使用那个新的URI 你可以通过这个响应代码来防止由于URI变更而导致老URI失效

**302("Found"):**

 临时重定向
**400("Bad Request")：**

服务器收到客户端通过PUT或者POST请求提交的表示，格式正确，但服务器不懂它什么意思

**401("Unauthorized")：**

客户端试图对一个受保护的资源进行操作（比如需要登录才可以看到一些信息），却又没有提供正确的用户密码

**403("Forbidden")：**

常用于一个资源只允许在特定时间段内访问，或者允许特定IP地址的用户访问的情况

**404("Not Found")：**

这也许是最广为人知的HTTP响应代码了。404表明服务器无法把客户端请求的URI转换为一个资源。web服务可以通过404响应告诉客户端所请求的URI是空的，然后客户端就可以通过向该URI发送PUT请求来创建一个新资源了。但是404也有可能是用来掩饰403或者401.

**405("Method Not Allowd"):**

客户端试图使用一个本资源不支持的HTTP方法。例如：一个资源只支持GET方法，但是客户端使用PUT方法访问

**500("Internal Server Error")：**

这是一个通用的服务器错误响应。对于大多数web框架，如果在执行请求处理代码时遇到了异常，它们就发送此响应代码

**502("Bad Gateway")：**

它表明代理方面出现问题，或者代理与上行服务器之间出现问题，而不是上行服务器本身有问题

**503("Service Unavailable")：**

服务器资源不足：服务器突然收到太多请求，以至于无法全部处理，所以选择拒绝接受客户端请求而不是接受它，并发送503响应代码。响应报头：服务器可以通过Retry-After报头告知客户端何时可以重试

**504("Gateway Timeout"):**

此响应代码表明代理无法连接上行服务器

**505("HTTP Version Not Supported")：**

当服务器不支持客户端试图使用的HTTP版本时发送此响应代码

## 16. BIO，NIO及IO多路复用

**BIO：**

当用户线程发出IO请求之后，内核会去查看数据是否就绪，如果没有就绪就会等待数据就绪，而用户线程就会处于阻塞状态，用户线程交出CPU。当数据就绪之后，内核会将数据拷贝到用户线程，并返回结果给用户线程，用户线程才解除block状态
**NIO：**

当用户线程发起一个read操作后，并不需要等待，而是马上就得到了一个结果。如果结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦内核中的数据准备好了，并且又再次收到了用户线程的请求，那么它马上就将数据拷贝到了用户线程，然后返回。用户线程需要不断地询问内核数据是否就绪，也就说非阻塞IO不会交出CPU，而会一直占用CPU

**IO多路复用：**

**select：**

各个客户端连接的文件描述符也就是套接字，都被放到了一个集合中，调用select函数之后内核会一直监视这些文件描述符中有哪些可读，如果有可读的描述符那么我们的工作进程就去读取资源把需要监听的文件描述符从用户空间拷贝fd_set（实际上是一个bitmap）到内核空间，内核去判断需要监听的文件描述符是否有数据，如果没有数据，select函数会阻塞。如果有数据，内核会把fd_set对应的bitmap的那一位置位，随后select函数返回，用户程序遍历fd_set，查看哪一位被置位，然后把对应的数据读出来。

缺点：

1.bitmap默认1024位，有限制

2.fd_set用了一次就不能再用了，必须重新创建一个bitmap

3.用户态切换到内核态，有开销

4.select返回时，不知道是哪一位被置位了，必须要再去遍历一遍bitmap，再去取对应的数据，这样时间复杂度是O（N）

**poll：**

poll 和 select 的实现非常类似，本质上的区别就是存放 fd 集合的数据结构不一样，poll使用pollfd结构而不是select的bitmap结构。有数据来的时候，内核会把poll中的revents字段置位，select 在一个进程内可以维持最多 1024 个连接，poll 在此基础上做了加强，可以维持任意数量的连接，原因是它是基于链表来存储的

**epoll：**

先用epoll_create创建一个epoll对象epfd，再通过epoll_ctl将需要监视的socket添加到epfd中，最后调用epoll_wait等待数据。epoll通过的事件注册函数epoll_control需要监听的文件描述符以及对应的事件封装在一个epfd结构体中，这个epfd是可以被内核和用户空间共享的，当有数据传过来的时候，select低效的另一个原因在于程序不知道哪些socket收到数据，只能一个个遍历。如果内核维护一个“就绪列表”，引用收到数据的socket，就能避免遍历。如下图所示，计算机共有三个socket，收到数据的sock2和sock3被rdlist（就绪列表）所引用。当进程被唤醒后，只要获取rdlist的内容，就能够知道哪些socket收到数据某个进程调用epoll_create方法时，内核会创建一个eventpoll对象（也就是程序中epfd所代表的对象）。eventpoll对象也是文件系统中的一员，和socket一样，它也会有等待队列，创建一个代表该epoll的eventpoll对象是必须的，因为内核要维护“就绪列表”等数据，“就绪列表”可以作为eventpoll的成员。创建epoll对象后，可以用epoll_ctl添加或删除所要监听的socket。以添加socket为例，如下图，如果通过epoll_ctl添加sock1、sock2和sock3的监视，内核会将eventpoll添加到这三个socket的等待队列中，当socket收到数据后，中断程序会操作eventpoll对象，而不是直接操作进程。当socket收到数据后，中断程序会给eventpoll的“就绪列表”添加socket引用。如下图展示的是sock2和sock3收到数据后，中断程序让rdlist引用这两个socket，eventpoll对象相当于是socket和进程之间的中介，socket的数据接收并不直接影响进程，而是通过改变eventpoll的就绪列表来改变进程状态。当程序执行到epoll_wait时，如果rdlist已经引用了socket，那么epoll_wait直接返回，如果rdlist为空，阻塞进程

**就绪列表的数据结构**

就绪列表引用着就绪的socket，所以它应能够快速的插入数据。程序可能随时调用epoll_ctl添加监视socket，也可能随时删除。当删除时，若该socket已经存放在就绪列表中，它也应该被移除。所以就绪列表应是一种能够快速插入和删除的数据结构。双向链表就是这样一种数据结构，epoll使用双向链表来实现就绪队列（对应上图的rdllist）

**索引结构**

既然epoll将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的socket。至少要方便的添加和移除，还要便于搜索，以避免重复添加。红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O(log(N))，效率较好。epoll使用了红黑树作为索引结构
**水平触发LT：**

当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据一次性全部读写完(如读写缓冲区太小)，那么下次调用 epoll_wait()时，它还会通知你在上没读写完的文件描述符上继续读写，当然如果你一直不去读写，它会一直通知你！！！如果系统中有大量你不需要读写的就绪文件描述符，而它们每次都会返回，这样会大大降低处理程序检索自己关心的就绪文件描述符的效率！！！

**边缘触发ET：**

当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用epoll_wait()时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你！！！这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符！！！